{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd9a89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from heisenberg_hqa import run_heisenberg_hqa_from_datafile, datafile_to_dataframe, test_hqa\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22834b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started:\n",
      "-----iteration 0--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(1.9994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 1--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(1.2963, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 2--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.9827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 3--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 4--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0974, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 5--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1273, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 6--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1655, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 7--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0873, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 8--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0814, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 9--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0478, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 10--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0477, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 11--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0193, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 12--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 13--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.5095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 14--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.8277, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 15--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0435, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 16--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 17--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.4019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 18--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.3569, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----iteration 19--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.2458, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 20--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1433, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 21--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1766, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 22--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 23--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0420, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 24--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.4319, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 25--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1483, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 26--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.4585, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 27--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0101, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 28--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.3753, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 29--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.3240, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 30--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 31--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.2577, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 32--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1287, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 33--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1317, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 34--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 35--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 36--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.3991, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 37--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.1427, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----iteration 38--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.0664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "-----iteration 39--model:HQA(\n",
      "  (c_decoder_layer1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (c_decoder_layer_out): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (c_layer1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (c_layer2): Linear(in_features=120, out_features=75, bias=True)\n",
      "  (c_out): Linear(in_features=75, out_features=25, bias=True)\n",
      ")------\n",
      "Loss tensor(0.2420, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = run_heisenberg_hqa_from_datafile(\"data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74395b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data = datafile_to_dataframe(\"data.pkl\")\n",
    "distributions = np.array(data.statevector.to_list())\n",
    "characteristics = data[['depth', 'hamiltonian_indx']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cae1fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.06986737500497459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the HQA\n",
    "test_results = test_hqa(model, 20, distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb75c22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ... i=0, chara=[1 0]\n",
      "Creating ... i=1, chara=[2 0]\n",
      "Creating ... i=2, chara=[3 0]\n",
      "Creating ... i=3, chara=[4 0]\n",
      "Creating ... i=4, chara=[1 1]\n",
      "Creating ... i=5, chara=[2 1]\n",
      "Creating ... i=6, chara=[3 1]\n",
      "Creating ... i=7, chara=[4 1]\n"
     ]
    }
   ],
   "source": [
    "# Creating the vectors\n",
    "model.create_latent_vectors(distributions, characteristics)\n",
    "# model.pca_transform_latent_vectors(3, True)\n",
    "model.dataframe_latent_points('heisenberg')\n",
    "model.latent_landscape(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17655665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12,5))\n",
    "ax = sns.scatterplot(x=\"p1\", y=\"p2\", hue='depth', data=model.df_latent_vectors, ax=axs[0])\n",
    "ax = sns.scatterplot(x=\"p1\", y=\"p2\", hue='ham_indx', data=model.df_latent_vectors, ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "024c02c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>ham_indx</th>\n",
       "      <th>latent_vector</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.75140464, 0.2200312, 0.07900041, 0.19230554...</td>\n",
       "      <td>-0.602086</td>\n",
       "      <td>0.368719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7550371, 0.2180929, 0.081800275, 0.19056703...</td>\n",
       "      <td>-0.605708</td>\n",
       "      <td>0.370795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7658887, 0.21519817, 0.07516377, 0.18180412...</td>\n",
       "      <td>-0.603039</td>\n",
       "      <td>0.372333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7658891, 0.21519797, 0.07516366, 0.18180424...</td>\n",
       "      <td>-0.603040</td>\n",
       "      <td>0.372333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.76929545, 0.21263947, 0.08492842, 0.1969406...</td>\n",
       "      <td>-0.630058</td>\n",
       "      <td>0.372063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.7688487, 0.21219191, 0.08592346, 0.19812849...</td>\n",
       "      <td>-0.631700</td>\n",
       "      <td>0.371870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.7688486, 0.2121919, 0.085923515, 0.19812857...</td>\n",
       "      <td>-0.631700</td>\n",
       "      <td>0.371870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.7688486, 0.21219192, 0.085923485, 0.1981284...</td>\n",
       "      <td>-0.631700</td>\n",
       "      <td>0.371870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   depth  ham_indx                                      latent_vector  \\\n",
       "0      1         0  [0.75140464, 0.2200312, 0.07900041, 0.19230554...   \n",
       "1      2         0  [0.7550371, 0.2180929, 0.081800275, 0.19056703...   \n",
       "2      3         0  [0.7658887, 0.21519817, 0.07516377, 0.18180412...   \n",
       "3      4         0  [0.7658891, 0.21519797, 0.07516366, 0.18180424...   \n",
       "4      1         1  [0.76929545, 0.21263947, 0.08492842, 0.1969406...   \n",
       "5      2         1  [0.7688487, 0.21219191, 0.08592346, 0.19812849...   \n",
       "6      3         1  [0.7688486, 0.2121919, 0.085923515, 0.19812857...   \n",
       "7      4         1  [0.7688486, 0.21219192, 0.085923485, 0.1981284...   \n",
       "\n",
       "         p1        p2  \n",
       "0 -0.602086  0.368719  \n",
       "1 -0.605708  0.370795  \n",
       "2 -0.603039  0.372333  \n",
       "3 -0.603040  0.372333  \n",
       "4 -0.630058  0.372063  \n",
       "5 -0.631700  0.371870  \n",
       "6 -0.631700  0.371870  \n",
       "7 -0.631700  0.371870  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.df_latent_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
